---
title: "Développement d’un modèle joint de distribution des espèces pour la réalisation d’une carte de biodiversité à Madagascar"
author: "Jeanne Clément"
date: "Rapport de stage, Février à Août 2019"
output:
  pdf_document :
    latex_engine : xelatex
    fig_caption : yes
    pandoc_args: ["--number-sections"]
header-includes: 
  - \usepackage{upgreek}
  - \usepackage{easybmat}
  - \usepackage{float}
  - \usepackage{fancyhdr}
  - \usepackage{hyperref}
  - \usepackage[frenchb]{babel}
  - \usepackage{amsmath,amssymb,amsthm}
  - \usepackage{graphicx}
  - \usepackage[labelfont=bf,textfont=sl,tableposition=top,small]{caption}
  - \usepackage{enumitem}
  - \usepackage{dsfont}
  - \usepackage{bbm}
  - \usepackage{answers}
  - \usepackage{xassoccnt}


bibliography: Biblio_master.bib
geometry: left=1.4cm,right=1.4cm,top=1.5cm,bottom=1.3cm
fontsize : 10pt
---

\newtheorem{prop}{Proposition}
\RemoveFromReset{prop}{section}
\AddToReset{prop}{subsubsection}
\renewcommand{\theprop}{\thesubsubsection .\arabic{prop}}
\theoremstyle{definition}
\newtheorem{defn}{Définition}
\renewcommand{\thedefn}{\thesubsubsection .\arabic{dfn} }
\newtheorem{dem}{Preuve}
\RemoveFromReset{dem}{section}
\AddToReset{dem}{subsubsection}
\renewcommand{\thedem}{\thesubsubsection.\arabic{dem}}
\newtheorem{thm}{Théorème}
\renewcommand{\thethm}{\thesubsubsection.\arabic{thm}}
\renewcommand{\contentsname}{Sommaire}
\renewcommand{\listtablename}{Liste des tableaux}
\renewcommand{\listfigurename}{Liste des figures}
\renewcommand{\baselinestretch}{1.5}

\thispagestyle{empty}
\begin{center}

Enseignant référent : Benoite De Saporta  

Encadrant : Ghislain Vieilledent  

\vspace{0.5cm}

```{r echo=FALSE, out.width='90%'}
knitr::include_graphics('Illustrations/Illustrations_Rapport/foret.jpg')

```

\vspace{0.5cm}

Master Maths-Biostatistique  

Université Montpellier 2  

UMR AMAP - Montpellier  

\vspace{1cm}

\includegraphics[height=1.7cm, width=1.7cm]{Illustrations/Illustrations_Rapport/logo_UM.jpg}
\includegraphics[height=2cm, width=2cm]{Illustrations/Illustrations_Rapport/logo-AMAP.png}
\includegraphics[height=1.3cm]{Illustrations/Illustrations_Rapport/titre-long.png}
\includegraphics[height=3.1cm, width=3.1cm]{Illustrations/Illustrations_Rapport/Logo-Cirad.png}
\end{center}

\newpage
\thispagestyle{empty}
\section*{Remerciements}
J'aimerai adresser mes plus sincères remerciements à G. Vieilledent qui m'a encadrée et conseillée durant ce stage riche en découvertes puisque le language C++, la construction de packages R ainsi que les modèles joints de distribution des espèces m'étaient inconnus. Il m'a beaucoup appris et encouragée à trouver des solutions par moi même. Je remercie également les chercheurs et autres stagiaires de l'UMR AMAP pour leur accueil chaleureux et leur bonne humeur communicative qui font du laboratoire un cadre de travail idéal et tout particulièrement G. Le Moguedec qui fut une référence précieuse en statistiques ainsi que l'instigateur de pic-niques au lac du Crès qui nous ont bien aidé à supporter la canicule. 

\newpage
\thispagestyle{empty}
\tableofcontents
\newpage
\thispagestyle{empty}
\listoffigures
\listoftables
\newpage
\thispagestyle{empty}
\setcounter{page}{1} 

\section*{Introduction}

J'ai effectué mon stage au sein de l'UMR AMAP (botAnique et Modélisation de l'Architecture des Plantes et des végétations), qui se trouve à Montpellier. Il s'agit d'une unité interdisciplinaire hébergée par le Cirad ou « Centre de Coopération Internationale en Recherche Agronomique pour le Développement» et qui mène des recherches sur les plantes et les végétations, dans le but de prévoir la réponse des écosystèmes aux forçages environnementaux.  

Ce stage s'inscrit dans le cadre du projet BioSceneMada qui vise à fournir des scénarios d'évolution de la biodiversité sous l’eﬀet conjoint du changement climatique et de la déforestation à Madagascar. Pour ce faire, plusieurs jeux de données sur la biodiversité ont été collectés et regroupés pour diﬀérents groupes taxonomiques (mammifères, oiseaux, reptiles, amphibiens, arbres, plantes herbacées, invertébrés), parmi lesquels j'ai utilisé des inventaires forestiers répertoriant l'absence ou la présence d'espèces d'arbres sur différents sites de l'île ainsi que des variables bioclimatques afin d'ajuster un modèle joint de distribution des espèces permettent d’estimer la niche des espèces, de prédire leur distribution, tout en prenant en compte les intéractions entre espèces (@Warton2015).  
Dans un premier temps j'ai implémenté différents échantillonneurs de Gibbs en C++ permettant d’estimer les paramètres de modèles joints de distribution des espèces (JSDM) comportant des variables latentes, à l'aide du package Rcpp. La construction du package R \url{https://ecology.ghislainv.fr/jSDM/} autour de l'une de ces fonctions ainsi que sa présentation à la conférence useR 2019 ont constitué une partie importante de mon stage. 
L'ajustement d'un JSDM sur des données d’inventaires forestiers collectées à Madagascar
ainsi que des variables climatiques et environnementales, m'a permis d’obtenir des carte reflétant la biodiversité sur l'île afin de par la suite identiﬁer des zones refuges de la biodiversité sous l’eﬀet du changement climatique en utilisant les variables bioclimatiques fournies par les scénarios du GIECC. 
Ces résultats seront utilisés pour des préconisations de gestion de la biodiversité dans le cadre du projet BioSceneMada.  

\newpage
\pagestyle{headings}

# Définition des modèles joints de distribution des espèces envisagés \quad

Les données dont on dispose pour ajuster ce type de modèle sont les réalisations d'une variable réponse,  
$Y=(y_{ij})^{i=1,\ldots,I}_{j=1,\ldots,J}$ telle que :

$$y_{ij}=\begin{cases}
    0 & \text{ si l'espèce $j$ est absente du site $i$}\\
    1 &  \text{ si l'espèce $j$ est présente sur le site $i$,}
    \end{cases}$$
ainsi que de variables explicatives $X=(X_i)_{i=1,\ldots,I}$ avec $X_i=(X_{i1},\ldots,X_{ip})\in \mathbb{R}^p$ où $p$ est le nombre de variables bioclimatiques considérées pour chaque site.  
On note $\theta_{ij}$, la probabilité de présence de l'espèce $j$ sur le site $i$.

L'article @Warton2015 développe deux approches hiérarchiques  pouvant être utilisées à la spécification d’un modèle joint de distribution des espèces.

## Modèle linéaire mixte généralisé (GLMM) 

D'une part on pourrait utiliser un modèle linéaire mixte généréralisé **(GLMM)** de la forme : 
$$g(\theta_{ij}) =\alpha_i + \beta_{j0} + X_i\beta_j + u_{ij},$$
$$y_{ij} \ | \ u_{ij}, \alpha_i \sim \mathcal{B}ernoulli(\theta_{ij}),$$
$$u_i \sim \mathcal{N}_J(0_{\mathbb{R}^J},\Sigma) \ iid,$$
$$\alpha_i \sim \mathcal{N}(0, V_{\alpha}) \ iid \text{ et indépendant de } u_i.$$
où $g : \ ]0,1[ \ \rightarrow \ ]-\infty, +\infty[$ est une fonction de lien, $\beta_j=(\beta_{j1},\ldots,\beta_{jp})'$ et $\beta_{j0}$ sont les coefficients de régression correspondants aux variables bioclimatiques et l'intercept pour l'espèce $j$ qui est supposé être un effet fixe, $\alpha_i$ représente l'effet aléatoire du site $i$, et $u_i=(u_{i1},\ldots,u_{iJ})$ est un effets aléatoires multivariés corrélés dont la matrice de variance covariance $\Sigma$ controle la correlation entre les espèces et est supposée être complètement non structurée.    
Cette dernière partie du modèle est problématique lorsque le nombre d'espèces $J$ est important car le nombre de paramètres dans $\Sigma$ augmente quadratiquement avec $J$.

## Modèle à variable latente (LVM)

D'autre part en posant $u_{ij} = W_i\lambda_j$, avec $W_i=(W_{i1},\ldots,W_{iq})$ les $q$ prédicteurs non mesurés (ou "variables latentes") considérés et $\lambda_j=(\lambda_{j1},\ldots, \lambda_{jq})'$ les coefficients associés, on obtient le modèle à variables latentes **(LVM)** suivant : 
$$g(\theta_{ij}) =\alpha_i + \beta_{j0} + X_i\beta_j + W_i\lambda_j$$
$$y_{ij} \ | \ W_i, \alpha_i \sim \mathcal{B}ernoulli(\theta_{ij}),$$
$$W_i \sim \mathcal{N}(0,I_q) \ iid $$
$$\alpha_i \sim \mathcal{N}(0,V_{\alpha}) \ iid \text{ et indépendant de } W_i$$ 

Ce qui revient à un cas particulier de GLMM multivarié auquel on impose la contrainte $\Sigma = \Lambda\Lambda'$ avec 
$$\Lambda := \begin{pmatrix}
\lambda_{11} & \ldots & \lambda_{1q} \\
\vdots & \vdots & \vdots \\
\lambda_{J1} & \ldots & \lambda_{Jq}
\end{pmatrix}$$

On préferera ce dernier modèle, en effet il comporte potentiellement beaucoup moins de paramètres que le GLMM précédent car $\Lambda$ a autant de colonne qu’il y a de variables latentes ($q$) tandis que $\Sigma$ présente autant de colonnes de paramètres qu’il y a d’espèces ($J$).

On peut choisir de modéliser l'abondance absolue plutôt que l'abondance relative en supprimant l'effet site aléatoire $\alpha_i$ du modèle. 

# Méthodes d’inférence bayesienne selon la fonction de lien choisie 

## Principe d’un échantillonneur de Gibbs

Dans le cadre bayésien, l’algorithme de Gibbs permet d’obtenir une réalisation du paramètre $\theta=(\theta_),\ldots,\theta_m)$ suivant la loi *a posteriori* $\uppi(\theta \ | \ x)$ dès que l’on est capable d’exprimer les lois conditionnelles : $\uppi(\theta_i \ | \ \theta_1,\dots,\theta_{i-1},\theta_{i+1},\ldots,\theta_m, x)$ pour $i =1,\ldots,m$.  

\vspace{0.2cm}

L’**échantillonnage de Gibbs** consiste à : 
\begin{itemize}
\item \textbf{Initialisation} : choix arbitraire de $\theta^{(0)}= (\theta_1^{(0)},\dots,\theta_m^{(0)})$.
\vspace{0.2cm}
\item \textbf{Itération $t$} : Générer $\theta^{(t)}$ de la manière suivante :
\vspace{0.2cm}
  \begin{itemize}
  \item[$\bullet$] $\theta_1^{(t)} \sim \uppi\left(\theta_1 \ | \theta_2^{(t-1)},\dots, \theta_m^{(t-1)}, x \right)$
\vspace{0.1cm}
  \item[$\bullet$] $\theta_2^{(t)} \sim \uppi\left((\theta_2 \ | \ (\theta_1^{(t)}, \theta_3^{(t-1)},\ldots,\theta_m^{(t-1)},x\right)$
  \item[$\bullet$] $\theta_m^{(t)} \sim \uppi\left(\theta_m \ | \ \theta_1^{(t)}, \ldots, \theta_{m-1}^{(t)},x\right)$
\end{itemize}
\end{itemize}

Les itérations successives de cet algorithme génèrent les états d’une chaîne de
Markov $\{\theta^{(t)}, t > 0\}$ à valeurs dans $\mathbb{R}^{m}$, on montre que cette chaîne admet une mesure invariante qui est la *loi a posteriori*.  
Pour un nombre d’itérations suffisamment grand, le vecteur $\theta$ obtenu peut donc être considéré comme étant une réalisation de la loi *a posteriori* $\uppi(\theta \ | \ x)$. 
\vspace{0.2cm} 

Par conséquent l'implémentation d'un échantillonneur de Gibbs nécessite la connaissance des ditributions *a posteriori* de chacun des paramètres conditionnellement aux autres paramètres du modèle, qui se déduisent des formules de priors conjugués dans le cas du modèle probit mais ne sont pas explicitement exprimables dans le cas où on utilise une fonction de lien logit.

## Modèle probit : échantillonneur de Gibbs et priors conjugués 

D’une part, on utilise un fonction de lien $\mathrm{probit} : p \rightarrow \Phi^{-1}(p)$ où $\Phi$ correspond à la fonction de répartition d’une loi normale centrée réduite.  

### Définition du modèle probit

D’après l’article @Albert1993, une modélisation possible est de supposer l’existence d’une variable latente sous-jacente liée à notre variable binaire observées en utilisant la proposition suivante :

\begin{prop}[Modèle probit par l'intermédiaire une variable latente]  \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad  \qquad \qquad \qquad  \qquad \qquad \qquad \qquad   

Si $Z_{ij} = \alpha_i + \beta_{j0}+X_i\beta_j+ W_i\lambda_j + \epsilon_{ij}, \ \forall i,j \text{ avec }  \epsilon_{i,j} \sim \mathcal{N}(0,1) \ \mathrm{iid}$ et tel que :
$$y_{i,j}=
\begin{cases}
1 & \text{ si } Z_{ij} > 0 \\
0 &  \text{sinon.}
\end{cases}$$
Alors on a $y_{ij} \ |\ Z_{ij} \sim \mathcal{B}ernoulli(\theta_{ij})$ avec
$\mathrm{probit(\theta_{ij})} = \alpha_i + \beta_{j,0}+X_i\beta_j+ W_i\lambda_j$.
\end{prop}
\begin{dem}   
$$\begin{aligned}
\mathbb{P}(y_{ij}=1) & = \mathbb{P}(Z_{ij} > 0)\\
& = \mathbb{P}(\alpha_i + \beta_{j0}+X_i\beta_j+ W_i\lambda_j + \epsilon_{ij} > 0)\\
& = \mathbb{P}(\epsilon_{ij} > - (\alpha_i + \beta_{j0} + X_i\beta_j + W_i\lambda_j) \ ) \\
& = \mathbb{P}(\epsilon_{ij} \leq \alpha_i + \beta_{j0} + X_i\beta_j + W_i\lambda_j) \\
& = \Phi( \alpha_i + \beta_{j0} + X_i\beta_j + W_i\lambda_j) \\
\end{aligned}$$
De la même façon on a :   
$$\begin{aligned}
\mathbb{P}(y_{ij}=0) & = \mathbb{P}(Z_{ij} \leq 0)\\
& = 1 - \Phi( \alpha_i + \beta_{j,0} + X_i\beta_j + W_i\lambda_j) \\
\end{aligned}$$
\end{dem}
On définit le modèle probit à l’aide d’une variable latente afin d’être en mesure d’uiliser les propriétés des priors conjugués pour échantillonner les paramètres du modèle selon leur distributions conditionnelles *a posteriori*. 

### Priors utilisés 

Afin d’utiliser une méthode d’inférence bayesienne on détermine une distribution *a priori* pour chacun des paramètres du modèle :
$$\begin{array}{lll}
V_{\alpha} & \sim & \mathcal {IG}(\text{shape}=0.5, \text{rate}=0.005) \text{ avec } \mathrm{rate}=\frac{1}{\mathrm{scale}}, \\
\beta_{jk} & \sim & \mathcal{N}(0,10^6)  \text{ pour } j=1,\ldots,J \text{ et } k=1,\ldots,p, \\
\lambda_{jl} & \sim & \begin{cases}
\mathcal{N}(0,10) & \text{si } l < j \\
\mathcal{N}(0,10) \text{ tronquée à gauche par } 0 & \text{si } l=j \\
P \text{ tel que } \mathbb{P}(\lambda_{jl} = 0)=1  & \text{si } l>j
\end{cases} \\
\quad & \quad & \text{ pour } j=1,\ldots,J \text{ et } l=1,\ldots,q.
\end{array}$$
En effet pour assurer l’identifiabilité du modèle les valeurs de $\Lambda$ sont contraintes à des valeurs strictements positives sur la diagonale et nulles au dessus de celle-ci, $\Lambda$ est ainsi supposée être triangulaire inférieure d’après l’article @Warton2015.  
La fonction *boral()* du package du même nom, permettant d'ajuster toutes sortes de modèles utilise ces distributions *a priori* pour le modèle qui nous intéresse. Dans l'article @Warton2015 l'ajustement de modèle joints de distributions des espèces est réalisé avec *boral()* qui fonctionne avec JAGS (Just Another Gibbs Sampler) un programme de simulation à partir de modèles hiérarchiques bayésiens utilisant des méthodes MCMC, implémenté en C++. Cependant la fonction *jSDM_probit_block()* du package jSDM que j'ai implémentée utilise une distribution *a priori* jointe pour les effets espèces fixes de la manière qui suit. 

### Propositions sur les priors conjugués  

**Effets espèces fixes**:  

On se ramène à un modèle de la forme $Z^* = X\beta + \epsilon$, en posant $Z^*_{i,j} = Z_{ij}  - \alpha_i = \beta_{j0} + X_i\beta_j+ W_i\lambda_j+\epsilon_{ij}$, afin d'estimer simultanément les $\beta_j$ et $\lambda_j$ pour chacune des espèces $j$,   ce qui revient en écriture matricielle à : 
$$\begin{aligned} 
Z^*_{j} = &\begin{pmatrix} 
Z^*_{1j} \\
\vdots \\
Z^*_{Ij}
\end{pmatrix} =  \underbrace{
\begin{pmatrix}
 1 & X_{11} & \ldots & X_{1p} & W_{11} & \ldots & W_{1q}\\
 \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
 1 & X_{I1} & \ldots & X_{Ip} & W_{I1} & \ldots & W_{Iq}\\
\end{pmatrix}}_{D}
\underbrace{
\begin{pmatrix}
\beta_{j0} \\
\beta_{j1} \\
\small{\vdots} \\
\beta_{jp} \\
\lambda_{j1} \\
\small{\vdots} \\
\lambda_{jq} 
\end{pmatrix}}_{P_j}
+ \begin{pmatrix} 
\epsilon_{1j} \\
\vdots \\
\epsilon_{Ij}
\end{pmatrix} \\
& =  DP_j + \epsilon_j \quad \text{ avec } \epsilon_j \sim \mathcal{N}_I(0_{\mathbb{R}^I},I_I).
\end{aligned}$$
On suppose que $P_j \sim \mathcal{N}_{p+q+1}(m,V)$ avec $m=0_{\mathbb{R}^{p+q+1}}$ et $V=diag(\underbrace{10^6,\ldots,10^6}_{\times p+1},\underbrace{10,\ldots,10}_{\times q})$, par exemple.  
Bien que cette distribution *a priori* ne prennent pas en compte les contraintes sur $\Lambda$, elle permet l'échantillonage selon une loi normale multivariée des effets espèce fixes. On imposera les contraintes aux $\lambda_{jl}$ concernés après les avoir simulés. 

On applique la proposition suivante : 

\begin{prop}
$$\begin{cases} 
Y \ | \ \beta \sim \mathcal{N}_n ( X\beta, I_n) \\
\beta  \sim \mathcal{N}_p (m,V)
\end{cases}
\Rightarrow \begin{cases}
\beta|Y \sim \mathcal{N}_p (m^*,V^*) \text{ avec }  \\
m^* = (V^{-1} + X'X)^{-1}(V^{-1}m + X'Y)\\
V^*=(V^{-1} + X'X)^{-1} 
\end{cases}$$.
\end{prop}
\begin{dem}
$$\begin{aligned}
p(\beta \ | \ Y) & \propto  p(Y \ | \ \beta) \ p(\beta) \\
& \propto  \frac{1}{(2\pi)^{\frac{n}{2}}}\exp\left(-\frac{1}{2}(Y-X\beta)'(Y-X\beta)\right)\frac{1}{(2\pi)^{\frac{p}{2}}|V|^{\frac{1}{2}}}\exp\left(-\frac{1}{2}(\beta-m)'V^{-1}(\beta-m)\right) \\
& \propto \exp\left(-\frac{1}{2}\left((\beta-m)'V^{-1}(\beta-m) + (Y-X\beta)'(Y-X\beta)\right)\right) \\
& \propto \exp\left(-\frac{1}{2}\left(\beta'V^{-1}\beta + m'V^{-1}m - m'V^{-1}\beta -\beta'V^{-1}m + Y'Y + \beta'X'X\beta - Y'X\beta - \beta'X'Y\right)\right) \\
& \propto \exp\left(-\frac{1}{2}\left(\beta'(V^{-1}+X'X)\beta -\beta'(V^{-1}m + X'Y) - (Y'X + m'V^{-1})\beta + m'V^{-1}m + Y'Y \right)\right) \\
& \propto \exp\left(-\frac{1}{2}\left(\beta'(V^{-1}+X'X)\beta -\beta'(V^{-1}m + X'Y) - (X'Y + V^{-1}m)'\beta + m'V^{-1}m + Y'Y \right)\right) \\
& \propto \exp(-\frac{1}{2}\left(\beta - (V^{-1}+X'X)^{-1}(V^{-1}m + X'Y)\right)'(V^{-1}+X'X)\left(\beta - (V^{-1}+X'X)^{-1}(V^{-1}m + X'Y)\right)\\
& \quad -(V^{-1}m + X'Y)'(V^{-1}+X'X)^{-1}(V^{-1}m + X'Y) +m'V^{-1}m + Y'Y)\\
& \propto \exp\left(-\frac{1}{2}\left(\beta - \underbrace{(V^{-1}+X'X)^{-1}(V^{-1}m + X'Y)}_{m^*}\right)'\underbrace{(V^{-1}+X'X)}_{{V^*}^{-1}}\left(\beta - (V^{-1}+X'X)^{-1}(V^{-1}m + X'Y)\right)\right)
\end{aligned}$$
\end{dem}
On obtient : 
$$\begin{cases} 
Z^*_j \ | \ P_j \sim \mathcal{N}_{I} ( DP_j, I_{I}) \\
P_j \sim \mathcal{N}_{p+q+1}(m,V)
\end{cases}
\Rightarrow \begin{cases}
P_j \ | \ Z^*_j  \sim \mathcal{N}_{p+q+1} (m^*,V^*) \text{ avec }  \\
m^* = (V^{-1} + D'D)^{-1}(V^{-1}m + D'Z^*_j)\\
V^*=(V^{-1} + D'D)^{-1} 
\end{cases}$$.

**Prédicteurs non mesurés ( ou “variables latentes”) : **

De la même façon, on pose : $Z^\star_{ij} = Z_{ij} - \alpha_i - \beta_{j0} - X_i\beta_j = W_i\lambda_j + \epsilon_{ij}$, afin d’estimer $W_i$ pour chaque site $i$.  
En appliquant la proposition précédente, on obtient :

$$\begin{cases} 
Z^*_i := (Z^*_{i1},\ldots,Z^*_{iJ})' \ | \ W_i \sim \mathcal{N}_{J} ( \Lambda W_i', I_{J}) \\
W_i' \sim \mathcal{N}_{q}(0_{\mathbb{R}^{q}},I_q)
\end{cases}
\Rightarrow \begin{cases}
W_i' \ | \ Z^*_i \sim \mathcal{N}_{q} (m^*,V^*) \text{ avec }  \\
m^* = (I_q + \Lambda'\Lambda)^{-1}(\Lambda'Z^*_i)\\
V^* = (I_q + \Lambda'\Lambda)^{-1} 
\end{cases}$$.

**Effets site aléatoires et variance associée : **   

En ce qui concerne l'effet site aléatoire $(\alpha_i)_{i=1,\dots,I}$, on pose $Z^*_{i,j} = Z_{ij} - D_iP_j = \alpha_i + \epsilon_{i,j}$, avec $D_i =(1,X_{i1},\ldots,X_{ip},W_{i1},\ldots,W_{iq})$. \smallskip   
On a ainsi $Z^*_{ij} \ | \ \alpha_i \ \sim \mathcal{N}(\alpha_i,1)$ *iid* pour $j=1,\ldots,J$, puis on applique la proposition suivante : 
\begin{prop}
$$\begin{cases} 
x_i \ | \ \theta \sim \mathcal{N}(\theta, \ \sigma^2) \ iid \text{ pour } i=1,\ldots,n\\
\theta \sim \mathcal{N}(\mu_0,{\tau_0}^2) \\
\sigma^2 \text{ connu}
\end{cases}
\Rightarrow
\begin{cases} 
\theta | \ x_1, \ldots,x_n \sim \mathcal{N}(\mu_1,{\tau_1}^2) \text{ avec } \\
\mu_1 = \dfrac{{\tau_0}^{-2}\mu_0 + \sigma^{-2}\sum_{i=1}^nx_i}{{\tau_0}^{-2}+n\sigma^{-2}} \\
{\tau_1}^{-2} = {\tau_0}^{-2}+n\sigma^{-2}
\end{cases}$$. 
\end{prop}
\begin{dem}
$$\begin{aligned}
p(\theta \ | \ x_1,\ldots,x_n) & \propto p(\theta) p(x_1,\ldots,x_n \ | \ \theta) \\
& \propto  \frac{1}{(2\pi{\tau_0}^2)^{\frac{1}{2}}}\exp\left(-\frac{1}{2{\tau_0}^2}(\theta-\mu_0)^2\right) \prod\limits_{i=1}^n\frac{1}{(2\pi\sigma^2)^{\frac{1}{2}}}\exp\left(-\frac{1}{2\sigma^2}(x_i-\theta)^2\right) \\
& \propto \exp\left(-\frac{1}{2{\tau_0}^2}(\theta-\mu_0)^2-\frac{1}{2\sigma^2}\sum\limits_{i=1}^n(x_i-\theta)^2\right) \\
& \propto \exp\left(-\frac{1}{2{\tau_0}^2}(\theta^2-2\mu_0\theta)-\frac{1}{2\sigma^2}\sum\limits_{i=1}^n(\theta^2-2\theta x_i)\right)\\
& \propto \exp\left(-\frac{1}{2}\left(\theta^2 ({\tau_0}^{-2}+n\sigma^{-2})-2\mu_0\theta{\tau_0}^{-2}-2\theta\sigma^{-2}\sum\limits_{i=1}^n x_i\right)\right)\\
& \propto \exp\left(-\frac{1}{2({\tau_0}^{-2}+n\sigma^{-2})^{-1}}\left(\theta -\frac{\mu_0{\tau_0}^{-2}+ \sigma^{-2}\sum\limits_{i=1}^n x_i}{{\tau_0}^{-2}+n\sigma^{-2}}\right)^2\right)\\
\end{aligned}$$
\end{dem}
On obtient ainsi : 
$$\begin{cases} 
Z^*_{ij} \ | \ \alpha_i \sim \mathcal{N}(\alpha_i, \ 1) \text{, iid } \forall j=1,\ldots,J\\
\alpha_i \sim \mathcal{N}(0,V_{\alpha}) \\
\end{cases}
\Rightarrow
\begin{cases} 
\alpha_i | \ Z^*_{i1}, \ldots, Z^*_{iJ} \sim \mathcal{N}(\mu_1,{\tau_1}^2) \text{ avec } \\
\mu_1 = \dfrac{ \sum_{j=1}^J Z^*_{ij}}{V_{\alpha}^{-1}+ J} \text{ et } {\tau_1}^{-2} = V_{\alpha}^{-1}+ J.
\end{cases}$$
Finalement pour estimer $V_{\alpha}$, la variance des effets site aléatoires $(\alpha_i)_{i=1,\dots,I}$, on utilise la proposition suivante :
\begin{prop}
Si $$\begin{cases} 
x \ | \ \sigma^2 \sim \mathcal{N}_n (\theta, \ \sigma^2I_n) \\
\sigma^2   \sim \mathcal{IG} (a,b) \\
\theta \text{ connu}
\end{cases} \Rightarrow 
\begin{cases}
\sigma^2|x \sim \mathcal{IG}(a',b') \text{ avec } \\
a' = a + \frac{n}{2} \text { et } b' = \frac{1}{2}\sum\limits_{i=1}^n(x_i-\theta)^2 + b. 
\end{cases}$$
\end{prop}
\begin{dem}
$$\begin{aligned}
p(\sigma^2 \ | \ x) & \propto  p(x \ | \ \sigma^2) \ p(\sigma^2) \\
& \propto  \frac{1}{(2\pi\sigma^2)^{\frac{n}{2}}}\exp\left(-\frac{1}{2\sigma^2}(x-\theta)'(x-\theta)\right)\frac{b^a}{\Gamma(a)}{(\sigma^2)}^{-(a+1)}\exp\left(-\frac{b}{\sigma^2}\right) \\
& \propto {(\sigma^2)}^{-\left(\frac{n}{2}+a+1\right)}\exp\left(-\frac{1}{\sigma^2}\left(b+\frac{1}{2}\sum\limits_{i=1}^n(x_i-\theta)^2\right)\right)
\end{aligned}$$
\end{dem}
On a donc : $$\begin{cases} 
(\alpha_1,\ldots,\alpha_I)' \ | \ V_{\alpha} \sim \mathcal{N}_n (0_{\mathbb{R}^I}, V_\alpha I_n) \\
V_\alpha \sim \mathcal{IG} (a,b) \\
\end{cases} \Rightarrow 
\begin{cases}
V_\alpha \ | \ \alpha_1,\ldots,\alpha_I \sim \mathcal{IG}(a',b') \text{ avec } \\
a' = a + \frac{I}{2} \text{ et } b' = b + \frac{1}{2}\sum\limits_{i=1}^I \alpha_i^2. 
\end{cases}$$

\newpage
### Echantilloneur de Gibbs 
L’algorithme utilisé pour estimer les paramètres du modèle logit est donc le suivant :
\begin{itemize}
\item Définir les constantes $N_{Gibbs}$, $N_{burn}$, $N_{thin}$ telles que $N_{Gibbs}$ correspond au nombre d’itérations effectuées par l’échantillonneur de Gibbs,  $N_{burn}$ au nombre d’itérations nécessaires pour le burn-in ou temps de chauffe et $N_{samp} = \dfrac{N_{Gibbs}-N_{burn}}{N_{thin}}$ au nombre de valeurs estimées retenues pour chaque paramètre. En effet on enregistre les paramètres estimés à certaines itérations, afin d’obtenir un échantillon de $N_{samp}$ valeurs distribuées selon la distribution a posteriori pour chacun des paramètres.
\vspace{0.2cm}
\item Initialiser tous les paramètres à $0$ par exemple, excepté les valeurs diagonales de $\Lambda$ initialisées à $1$ et $V_{\alpha}^{(0)}=1$.
\vspace{0.2cm}
\item Gibbs sampler : à chaque itération $t$ pour $t=1,\ldots,N_{Gibbs}$ on répète chacune de ces étapes :
  \begin{itemize}
  \item[$\bullet$] Génerer la \textbf{variable latente} $Z^{(t)}=\left(Z_{ij}^{(t)}\right)_{i=1,\ldots,I}^{j=1,\ldots,J}$ telle que
$$Z_{ij}^{(t)} \sim  \begin{cases} 
\mathcal{N}\left(\alpha_i^{(t−1)} + \beta_{j0}^{(t−1)} + X_i\beta_j{(t−1)} + W_i^{(t−1)}\lambda_j^{(t−1)}, \ 1 \right) \text{ tronquée à droite par } 0 & \text{si } y_{ij } =0 \\ 
\mathcal{N}\left(\alpha_i^{(t−1)} + \beta_{j0}^{(t−1)} + X_i\beta_j{(t−1)} + W_i^{(t−1)}\lambda_j^{(t−1)}, \ 1 \right) \text{ tronquée à gauche par } 0 &        \text{si } y_{ij} =1
\end{cases}$$
, la variable latente est ainsi initialisée à la première itération en la générant selon ces lois normales centrées.
\vspace{0.1cm}
  \item[$\bullet$] Générer les \textbf{effets espèces fixes} $P_j^{(t)}=(\beta_{j0}^{(t)},\beta_{j1}^{(t)} \ldots, \beta_{jp}^{(t)},\lambda_{j1}^{(t)},\ldots, \lambda_{jq}^{(t)})'$ pour $j=1,\ldots,J$ selon : 
$$P_j^{(t)} \ | \ Z^{(t)}, W_1^{(t-1)}, \alpha_1^{(t-1)}, \ldots, W_I^{(t−1)}, \alpha_I^{(t-1)} \sim \mathcal{N}_{p+q+1}(m^\star,V^\star) \text{, avec }$$
$$m^\star = (V^{-1} + {D^{(t)}}'D^{(t)})^{-1}(V^{-1}m + {D^{(t)}}'Z^\star_j) \text{ et } V^\star = \left(V^{-1}+{D^{(t)}}'D^{(t)}\right)^{-1},$$
$$\text{où } Z_j^\star =(Z_{1j}^\star,\ldots,Z_{Ij}^\star)' \text{ tel que } Z^\star_{ij} = Z_{ij}^{(t)}-\alpha_i^{(t-1)}.$$
Afin de contraindre les valeurs diagonales de $\Lambda =\left(\lambda_{jl}\right)_{j=1,\ldots,J}^{l=1,\ldots,q}$ à des valeurs positives et de rendre la
matrice triangulaire inférieure, on modifie les valeurs des $P^{(t)}$ simulées aléatoirement selon les conditions suivantes :
$$P_{jp+1+l}^{(t)} = \lambda_{jl}^{(t)} \leftarrow \begin{cases}
0 & \text{si } l>j \\
\lambda_{jl}^{(t-1)} & \text{si } l=j \text{ et } \lambda_{jl}^{(t)} < 0.
\end{cases}$$ 
On pose $P^{(t)}=\left( P_1^{(t)} | \ldots | P_J^{(t)} \right)$.  
\vspace{0.1cm}

  \item[$\bullet$] Générer les \textbf{prédicteurs non mesurés} (ou “variables latentes”) $W_i^{(t)}$ pour $i=1,\ldots,I$ selon : $$W_i^{(t)} \ | \ Z^{(t)}, P^{(t)},  \alpha_i^{(t-1)} \sim \mathcal{N}_{q} \left((I_q + {\Lambda^{(t)}}'\Lambda^{(t)})^{-1}({\Lambda^{(t)}}'Z_i^{\star \star}),(I_q + {\Lambda^{(t)}}'\Lambda^{(t)})^{-1}\right),$$
$$\text{où } Z_i^{\star \star} =(Z_{i1}^{\star \star},\ldots,Z_{iJ}^{\star \star}) \text{ tel que } Z_{ij}^{\star \star } = Z_{ij}^{(t)}-\alpha_i^{(t-1)} − \beta_{j0}^{(t)} - X_i\beta_j^{(t)}.$$ On pose $D_i^{(t)} = \left(1,X_{i1},\ldots,X_{ip},W_{i1}^{(t)}, \ldots, W_{iq}^{(t)} \right)$.

  \item[$\bullet$] Générer les \textbf{effets sites aléatoires} $\alpha_i^{(t)}$ pour $i=1,\ldots,I$ selon :
$$ \alpha_i | \ Z^{(t)}, P^{(t)},W_i^{(t)} \sim \mathcal{N}\left(\dfrac{ \sum_{j=1}^J Z_{ij}^{(t)} - D_i^{(t)}P_j^{(t)}}{{V_{\alpha}^{(t-1)}}^{-1} + J} , \left( \frac{1}{V_{\alpha}^{(t-1)}}+ J \right)^{-1}  \right)$$

  \item[$\bullet$] Générer la \textbf{variance des effets site aléatoires} $V_\alpha^{(t)}$ selon : $$V_\alpha^{(t)} \ | \ \alpha_1^{(t)},\ldots,\alpha_I^{(t)} \sim \mathcal{IG}\left( \text{shape}=0.5 + \frac{I}{2}, \text{rate}=0.005 + \frac{1}{2}\sum\limits_{i=1}^I \left(\alpha_i^{(t)}\right)^2\right)$$
  \end{itemize}
\end{itemize}

\newpage 
## Modèle logit : échantillonneur de Gibbs et algorithme de Metropolis adaptatif  

D'autre part on considère une fonction de lien $\mathrm{logit}: p \rightarrow\ln\left(\frac{p}{1-p}\right)=\mathrm{F}^{-1}(p)$, avec $\mathrm{F}:x\rightarrow \frac{1}{1+e^{-x}}$ la fonction de répartition appelée sigmoïde d’une loi logistique standard. 
 
### Définition du modèle logit
De la même façon que pour le modèle probit, d’après @Givord2016 on peut définir le modèle logit par l’intermediaire d’une variable latente : $Z_{ij}= \alpha_i + \beta_{j0} + X_i\beta_j + W_i\lambda_j + \epsilon_{ij}$ pour $i=1,\ldots,I$ et $j=1,\ldots,J$, avec $\epsilon_{ij} \sim \mathrm{logistique}(0,1)$ *iid* et telle que : 
$$y_{i,j}=
\begin{cases}
1 & \text{ si } Z_{ij} > 0 \\
0 &  \text{sinon.}
\end{cases}$$ 
Cependant dans ce cas les distributions *a priori* de la variable latente et des paramètres n’étant pas conjuguées, on n’est pas en mesure d’utiliser les propriétés des priors conjugués donc la modélisation à l’aide d’une variable latente ne présente pas d’intérêt.    

Dans ce cas on suppose que $$y_{ij} \ | \theta_{ij} \sim \mathcal{B}(n_i,\theta_{ij})$$, avec
$\mathrm{probit(\theta_{ij})} = \alpha_i + \beta_{j,0}+X_i\beta_j+ W_i\lambda_j$ et $n_i$ le nombre de visites du site $i$.    

Par conséquent on échantillonnera les paramètres de ce modèle selon une estimation de leurs distributions conditionnelles *a posteriori* à l’aide d’un algorithme de Metropolis adaptatif.  

### Priors utilisés 
On détermine une distribution *a priori* pour chacun des paramètres du modèle :  

$$\begin{array}{lll}
V_{\alpha} & \sim & \mathcal {IG}(\text{shape}=0.5, \text{rate}=0.005) \text{ avec } \mathrm{rate}=\frac{1}{\mathrm{scale}}, \\
\beta_{jk} & \sim & \mathcal{N}(0,10^6)  \text{ pour } j=1,\ldots,J \text{ et } k=1,\ldots,p, \\
\lambda_{jl} & \sim & \begin{cases}
\mathcal{N}(0,10) & \text{si } l < j \\
\mathcal{U}(0,10) &  \text{si } l=j \\
P \text{ tel que } \mathbb{P}(\lambda_{jl} = 0)=1  & \text{si } l>j
\end{cases} \\
\quad &  \quad & \text{ pour } j=1,\ldots,J \text{ et } l=1,\ldots,q.
\end{array}$$

### Principe d'un algorithme de Metropolis adaptatif 

Cet algorithme appartient aux méthode MCMC et permet d’obtenir une réalisation du paramètre $\theta=(\theta_),\ldots,\theta_m)$ selon leurs distributions conditionnelles *a posterirori* $\uppi(\theta_i \ | \ \theta_1,\dots,\theta_{i-1},\theta_{i+1},\ldots,\theta_m, x)$, pour $i =1,\ldots,m$ connues à une constante multiplicative près.  
On le qualifie d'adaptatif car la variance de la densité instrumentale conditionnelle utilisée est adaptée en fonction du nombre d'acceptation lors des dernières itérations. 

\begin{itemize}
\item \textbf{Initialisation} : $\theta^{(0)}= (\theta_1^{(0)},\ldots,\theta_m^{(0)})$ fixés arbitrairement, les nombres d'acceptation $(n^A_{i})_{i=1,\ldots,m}$ sont intialisés à $0$ et les variances $(\sigma^2_i)_{i=1,\ldots,m}$ sont intialisées à $1$.
\item \textbf{Itération t} : pour $i=1,\ldots,m$
  \begin{itemize}
  
  \item[$\bullet$] Générer $\theta_i^\star \sim q(\theta_i^{(t-1)},.)$, avec une densité instrumentale conditionnelle $q(\theta_i^{(t-1)},\theta_i^\star)$ symétrique, on choisira une loi $\mathcal{N}(\theta_i^{(t-1)},{\sigma^2_{i}})$ par exemple.
  
  \item[$\bullet$] Calculer la probabilité d'acceptation : 
  $$\gamma=  min\left(1,\dfrac{\uppi\left(\theta_i^\star \ | \ \theta_1^{(t-1)},\dots,\theta_{i-1}^{(t-1)},\theta_{i+1}^{(t-1)},\ldots,\theta_m^{(t-1)}, x \right)}{\uppi\left(\theta_i^{(t-1)} \ | \ \theta_1^{(t-1)},\dots,\theta_{i-1}^{ (t-1)},\theta_{i+1}^{(t-1)},\ldots,\theta_m^{(t-1)},x\right)}\right)$$.
  
  \item[$\bullet$] Retenir $$\theta_i^{(t)} =  
  \begin{cases} 
  \theta_i^\star & \text{ avec probabilité } \gamma \\
  &\text{ si on est dans ce cas le nombre d'acceptation devient : } n^A_{i} \leftarrow n^A_{i} +1 \\
  \theta_i^{(t-1)} & \text{ avec probabilité } 1-\gamma. \\
  \end{cases}$$
  \end{itemize}
  
\item \textbf{Durant le burnin}, toutes les $\mathrm{DIV}$ itérations, avec 
$$\mathrm{DIV} =  \begin{cases} 
100 & \text{ si } N_{Gibbs} \geq 1000 \\
\dfrac{N_{Gibbs}}{10}& \text{sinon }  \\
\end{cases}$$
, où $N_{Gibbs}$ est le nombre total d'itération effectuées.   

On modifie les variances en fonction des nombres d'acceptation de la manière suivante pour $i=1,\ldots,m$ : 

\begin{itemize}
  \item[$\bullet$] On calcule le taux d'acceptation : $r^A_{i} = \dfrac{ n^A_i}{\mathrm{DIV}}$.
  \item[$\bullet$] On adapte les variances selon le taux d'acceptation et une constante fixée $R_{opt}$ : $$\sigma_i \leftarrow \begin{cases}  
\sigma_i\left(2-\dfrac{1-r^A_i}{1-R_{opt}}\right) & \text{ si } r^A_{i} \geq R_{opt} \\ \\
\dfrac{\sigma_i}{2-\dfrac{1-r^A_i}{1-R_{opt}}} & \text{ sinon }
\end{cases}$$  
  \item[$\bullet$] On réinitialise les nombres d'acceptation : $n^A_i \leftarrow 0$.  
  \end{itemize} 
\item Toutes les $\dfrac{N_{Gibbs}}{10}$ itérations, on calcule et affiche les taux d'acceptation moyen $m^A = \dfrac{1}{m}\sum\limits_{i=1,\ldots,m}r^A_i$.
\end{itemize}

### Echantilloneur de Gibbs et algorithme de Metropolis adapatatif

On utilise un algorithme de Metropolis adapatatif pour échantillonner les paramètres du modèle selon leurs distributions conditionnelles *a posteriori* estimées à une constante multiplicative près.

Dans un premier temps on définit la fonction $f$ permettant de calculer la vraisemblance du modèle en fonction des paramètres estimés :
$$ f : \lambda_j,\beta_{j0},\beta_j,\alpha_i, W_i, X_i, y_{ij},n_i \rightarrow  f(\lambda_j,\beta_{j0},\beta_j,\alpha_i, W_i, X_i, y_{ij},n_i)=\mathrm{L}(\theta_{ij})$$
\begin{itemize}
\item Calcule de $\mathrm{probit}(\theta_{ij})= \alpha_i + \beta_{j0} + X_i\beta_j + W_i\lambda_j$.
\item Calcule de $\theta_{ij}= \dfrac{1}{1+\exp\left(-\mathrm{probit}(\theta_{ij})\right)}$.
\item Retourne $\mathrm{L}(\theta_{ij})= p(y_{ij} \ | \ \theta_{ij},n_i)= \dbinom{n_i}{y_{ij}}(\theta_{ij})^{y_{ij}}(1-\theta_{ij})^{n_i-y_{ij}}$.
\end{itemize}
On répète ces étapes pour $i=1,\ldots,I$ et $j=1,\ldots,J$, et on pose $\theta = \left(\theta{ij}\right)_{i=1,\ldots I}^{j= 1,\ldots,J}$.  

On peut ainsi calculer la vraisemblance du modèle : $\mathrm{L}(\theta)= \sum\limits_{\substack{1\leq i\leq I \\   1 \leq j\leq I}}\mathrm{L}(\theta_{ij})$.  

D’après la formules de Bayes on a $$\mathrm{p}(\theta \ |  \ Y) \propto \uppi(\theta) \mathrm{L}(\theta).$$
On utilise donc les relations suivantes pour approcher les densités conditionnelles a posteriori de chacun des paramètres avec $\uppi(.)$ les densités correspondants à leurs lois *a priori*.

$$\begin{aligned}
& p(\beta_{jk} \ |  \ \beta_{j0},\beta_{j1},\ldots,\beta_{jk-1},\beta_{jk+1},\ldots,\beta_{jp}, \lambda_j,\alpha_1,\ldots,\alpha_I, W_1,\ldots,W_I,Y) \propto \uppi(\beta_{jk})\prod\limits_{1\leq i\leq I}  \mathrm{L}(\theta_{ij})\\
&p(\lambda_{jl} \ |  \ \lambda_{j1},\ldots,\lambda_{jl-1},\lambda_{jl+1},\ldots,\lambda_{jq}, \beta_j,\beta_{j0},\alpha_1,\ldots,\alpha_I, W_1,\ldots,W_I,Y) \propto  \uppi(\lambda_{jl}) \prod\limits_{1\leq i \leq I}\mathrm{L}(\theta_{ij})\\
&p(W_{il} \ |  \ W_{i1},\ldots,W_{il-1},W_{il+1},\ldots,W_{iq},\alpha_i,\beta_{10},\ldots,\beta_{J0},\beta_1,\ldots,\beta_J,\lambda_1,\ldots, \lambda_J,Y) \propto \uppi(W_{il}) \prod\limits_{1\leq j\leq J}\mathrm{L}(\theta_{ij})\\
&p(\alpha_i \ |  \ W_i,\beta_{10},\ldots,\beta_{J0},\beta_1,\ldots,\beta_J,\lambda_1,\ldots, \lambda_j,V_{\alpha},Y) \propto \uppi(\alpha_i \ | \ V_{\alpha}) \prod\limits_{1\leq j\leq J}\mathrm{L}(\theta_{ij})\\
& \text{, pour $i=1,\ldots,I$, $j=1,\ldots,J$, $k=1,\ldots,p$ et $l=1,\ldots,q$. 
}
\end{aligned}$$

\newpage
L’algorithme utilisé pour estimer les paramètres du modèle logit est donc le suivant :

\begin{itemize}
\item Définition des constantes $N_{Gibbs}$, $N_{burn}$, $N_{thin}$ et $R_{opt}$ tels que  $N_{Gibbs}$ correspond au nombre d'itérations effectuées par l'algorithme,  $N_{burn}$ au nombre d'itérations nécessaires pour le burn-in ou temps de chauffe,   
$N_{samp}= \dfrac{N_{Gibbs}-N_{burn}}{N_{thin}}$ correspondant au nombre de valeurs estimées retenues pour chaque paramètre. En effet on enregistre les paramètres estimés à certaines itérations afin d'obtenir $N_{samp}$ valeurs, nous permettant de représenter une distribution a posteriori pour chacun des paramètres.   
On fixe $R_{opt}$ le ratio d'acceptation optimal utilisé dans les algorithmes de Metropolis adaptatifs implémentés pour chacun des paramètres du modèle. 

\item Initialiser tous les paramètres à $0$ par exemple, excepté les valeurs diagonales de $\Lambda$ initialisées à $1$ et $V_{\alpha}^{(0)}=1$. Le nombre d'acceptation de chaque paramètre est intialisé à $0$ et les variances de leur densités instrumententales conditionnelles prennent la valeur $1$.
\item Gibbs sampler : à chaque itération $t$ pour $t=1,\ldots,N_{Gibbs}$ on répète chacune de ces étapes :
  \begin{itemize}
\vspace{0.1cm}
\item[$\bullet$] Générer les \textbf{effets sites aléatoires} $\alpha_i^{(t)}$ pour $i=1,\ldots,I$ selon un algorithme de Metropolis adaptatif simulant $\alpha_i^\star \sim \mathcal{N}(\alpha_i^{(t-1)},\sigma_{\alpha_i}^2)$ puis calculant le taux d'acceptation de la manière suivante :  
$$\gamma =min\left(1, \ \dfrac{\uppi\left(\alpha_i^\star \ | \ V_{\alpha}^{(t-1)}\right)\prod\limits_{1\leq j\leq J}f\left(\alpha_i^\star, W_i^{(t-1)},\beta_{j0}^{(t-1)},\beta_j^{(t-1)}, \lambda_j^{(t-1)}, X_i,y_{ij},n_i\right)}{\uppi\left(\alpha_i^{(t-1)} \ | \ V_{\alpha}^{(t-1)}\right)\prod\limits_{1\leq j\leq J}f\left(\alpha_i^{(t-1)}, W_i^{(t-1)},\beta_{j0}^{(t-1)},\beta_j^{(t-1)}, \lambda_j^{(t-1)}, X_i,y_{ij},n_i\right)}\right).$$

\item[$\bullet$] Générer la \textbf{variance des effets site aléatoires} $V_\alpha^{(t)}$ selon : $$V_\alpha^{(t)} \ | \ \alpha_1^{(t)},\ldots,\alpha_I^{(t)} \sim \mathcal{IG}\left( \text{shape}=0.5 + \frac{I}{2}, \text{rate}=0.005 + \frac{1}{2}\sum\limits_{i=1}^I \left(\alpha_i^{(t)}\right)^2\right)$$

\item[$\bullet$] Générer les \textbf{prédicteurs non mesurés} (ou “variables latentes”) $W_{il}^{(t)}$ pour $i=1,\ldots,I$ et $l=1,\ldots,q$ selon un algorithme de Metropolis adaptatif  simulant $W_{il}^\star \sim \mathcal{N}(W_{il}^{(t-1)}, \sigma_{W_{il}}^2)$ puis calculant le taux d'acceptation de la manière suivante :

$$\gamma = min\left(1,\ \dfrac{\uppi\left(W_{il}^\star\right)\prod\limits_{1\leq j\leq J}f\left(W_{il}^\star, \alpha_i^{(t)},\beta_{j0}^{(t-1)},\beta_j^{(t-1)}, \lambda_j^{(t-1)},X_i,y_{ij},n_i\right)} {\uppi\left(W_{il}^{(t-1)}\right)\prod\limits_{1\leq j\leq J}f\left(W_{il}^{(t-1)}, \alpha_i^{(t)},\beta_{j0}^{(t-1)},\beta_j^{(t-1)}, \lambda_j^{(t-1)}, X_i,y_{ij},n_i\right)}\right).$$

\item[$\bullet$] Générer les \textbf{effets espèces fixes} $\beta_{jk}^{(t)}$ pour $j=1,\ldots,J$ et $k=0,\ldots,p$ selon un algorithme de Metropolis adaptatif  simulant $\beta_{jk}^\star \sim \mathcal{N}(\beta_{jk}^{(t-1)}, \sigma_{\beta_{jk}}^2)$ puis calculant le taux d'acceptation de la manière suivante :

$$\gamma = min\left(1,\dfrac{\uppi\left(\beta_{jk}^\star\right)\prod\limits_{1\leq i\leq I}f\left(\beta_{j0}^{(t)},\small{\ldots},\beta_{jk-1}^{(t)},\beta_{jk}^\star,\beta_{jk+1}^{(t-1)},\small{\ldots}, \beta_{jp}^{(t-1)},\lambda_j^{(t-1)}, \alpha_1^{(t)},W_1^{(t)},\small{\ldots},\alpha_I^{(t)},  W_I^{(t)},X_i,y_{ij},n_i\right)} {\uppi\left(\beta_{jk}^{(t-1)}\right)\prod\limits_{1\leq i\leq I}f\left(\beta_{j0}^{(t)},\small{\ldots},\beta_{jk-1}^{(t)},\beta_{jk}^{(t-1)},\beta_{jk+1}^{(t-1)},\small{\ldots}, \beta_{jp}^{(t-1)},\lambda_j^{(t-1)}, \alpha_1^{(t)},W_1^{(t)}, \small{\ldots},\alpha_I^{(t)},  W_I^{(t)},X_i,y_{ij},n_i\right)}\right).$$
  
\item[$\bullet$] Générer les \textbf{effets espèces fixes liés aux prédicteurs non mesurés} $\lambda_{jl}^{(t)}$ pour $j=1,\ldots,J$ et $l=1,\ldots,q$ selon un algorithme de Metropolis adaptatif pour $l \geq j$, simulant $\lambda_{jl}^\star \sim \mathcal{N}(\lambda_{jl}^{(t-1)},\sigma_{\lambda_{jl}}^2)$ puis calculant le taux d'acceptation de la manière suivante : 
$$\gamma = min\left(1,\dfrac{\uppi\left(\lambda_{jl}^\star\right)\prod\limits_{1\leq i\leq I}f\left(\lambda_{j1}^{(t)},\small{\ldots},\lambda_{jl-1}^{(t)},\lambda_{jl}^\star,\lambda_{jl+1}^{(t-1)},\small{\ldots}, \lambda_{jq}^{(t-1)},\beta_{j0}^{(t)},\beta_j^{(t)}, \alpha_1^{(t)},W_1^{(t)},\small{\ldots},\alpha_I^{(t)},  W_I^{(t)},X_i,y_{ij},n_i\right)} {\uppi\left(\lambda_{jl}^{(t-1)}\right)\prod\limits_{1\leq i\leq I}f\left(\lambda_{j1}^{(t)},\small{\ldots},\lambda_{jl-1}^{(t)},\lambda_{jl}^{(t-1)},\lambda_{jl+1}^{(t-1)},\small{\ldots}, \lambda_{jq}^{(t-1)},\beta_{j0}^{(t)},\beta_j^{(t)}, \alpha_1^{(t)},W_1^{(t)},\small{\ldots},\alpha_I^{(t)},  W_I^{(t)},X_i,y_{ij},n_i\right)}\right).$$
Dans le cas $l>j$, on pose $\lambda_{jl}^{(t)} = 0$. 
\end{itemize}
\end{itemize}

\newpage 
## Evaluation de la fiabilité de ces méthodes sur des données simulées 

\newpage
# Application aux données collectées à Madagascar

## Description des données 

On dispose d'inventaires forestiers réalisés sur différents sites de l'île de Madagascar (plot sites).

## Estimation des paramètres
## Prédictions par interpolation 
## Prédictions avec auto-corrélation spatiale
## Analyse des résultats et mise en évidence de lieux refuges de la biodiversité

\section*{Conclusion}

\newpage
\pagestyle{empty}
\section*{References}